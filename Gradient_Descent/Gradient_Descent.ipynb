{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearFunction(W,b,x):\n",
    "    return (W*x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean square Error/ Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanSquareError(W,b,x,y): #loss function\n",
    "    return np.mean((LinearFunction(W,b,x) - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Function\n",
    "\n",
    "change in Loss function with respect to W = x * Error,\n",
    "\n",
    "change in Loss function with respect to b = Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(W,b,x,y): #mse derivatives\n",
    "    return np.mean(x*(W*x+b-y), axis=-1), np.mean(W*x+b-y, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_gradient_descent(W,b,x,y,lr = 1e-5, epsilon = 1e-4):\n",
    "    prev_error = 0\n",
    "    error = np.array([])\n",
    "    while True:\n",
    "        gradient_W, gradient_b = gradient(W,b,x,y)\n",
    "\n",
    "        if abs(MeanSquareError(W, b, x, y) - prev_error) < epsilon:\n",
    "            break\n",
    "            \n",
    "        prev_error = MeanSquareError(W,b,x,y)\n",
    "        error = np.insert(error, len(error), prev_error)\n",
    "\n",
    "        W -= lr * gradient_W\n",
    "        b -= lr * gradient_b\n",
    "        \n",
    "    return a, b, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_gradient_descent(W,b,x,y,lr=1e-5,momentum=0.9,epsilon=1e-4, batch_size=0):\n",
    "    if batch_size == 0: batch_size = len(x)\n",
    "    prev_grad_W = 0\n",
    "    prev_grad_b = 0\n",
    "    prev_error = 0\n",
    "    error = np.array([])\n",
    "    while True:\n",
    "        x_shuffled, y_shuffled = shuffle(x,y)\n",
    "        gradient_W, gradient_b = gradient(W,b,x_shuffled[:batch_size],y_shuffled[:batch_size])\n",
    "\n",
    "        if abs(MeanSquareError(W, b, x_shuffled, y_shuffled) - prev_error) < epsilon:\n",
    "            break\n",
    "        prev_error = MeanSquareError(W,b,x_shuffled,y_shuffled)\n",
    "        error = np.insert(error, len(error), prev_error)\n",
    "\n",
    "        W -= lr * gradient_W + momentum * prev_grad_W\n",
    "        b -= lr * gradient_b + momentum * prev_grad_b\n",
    "        \n",
    "        prev_grad_W = lr * gradient_W + momentum * prev_grad_W\n",
    "        prev_grad_b = lr * gradient_b + momentum * prev_grad_b\n",
    "        \n",
    "    return W, b, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAGRAD\n",
    " -Adaptive technique on basis of how gradient has been changing for all previous iterations, we try to change the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad_gradient_descent(W, b, x, y, lr=1e-5, epsilon=1e-4):\n",
    "    prev_error = 0\n",
    "    adagrad_W = 0\n",
    "    adagrad_b = 0\n",
    "    error = np.array([])\n",
    "    while True:\n",
    "        gradient_W, gradient_b = gradient(W, b, x, y)\n",
    "\n",
    "        if abs(MeanSquareError(W, b, x, y) - prev_error) < epsilon:\n",
    "            break\n",
    "        prev_error = MeanSquareError(W, b, x, y)\n",
    "        error = np.insert(error, len(error), prev_error)\n",
    "\n",
    "        adagrad_W += gradient_W**2\n",
    "        adagrad_b += gradient_b**2\n",
    "        W -= (lr / (adagrad_W**0.5 + 1e-8)) * gradient_W\n",
    "        b -= (lr / (adagrad_b**0.5 + 1e-8)) * gradient_b\n",
    "        \n",
    "    return W, b, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Prop\n",
    "\n",
    "- Damps out oscillation in vertical direction and so converges quickly\n",
    "- Allows use of larger value of learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop_gradient_descent(a, b, x, y, lr=1e-5, gamma=0.9, epsilon=1e-4):\n",
    "    prev_error = 0\n",
    "    rmsprop_a = 0\n",
    "    rmsprop_b = 0\n",
    "    error = np.array([])\n",
    "    while True:\n",
    "        gradient_a, gradient_b = gradient(a, b, x, y)\n",
    "#         print(abs(mse(a, b, x, y) - prev_error))\n",
    "        if abs(MeanSquareError(a, b, x, y) - prev_error) < epsilon:\n",
    "            break\n",
    "        prev_error = MeanSquareError(a, b, x, y)\n",
    "        error = np.insert(error, len(error), prev_error)\n",
    "\n",
    "        rmsprop_a = gamma * rmsprop_a + (1-gamma) * (gradient_a**2)\n",
    "        rmsprop_b = gamma * rmsprop_b + (1-gamma) * (gradient_b**2)\n",
    "        a -= (lr / (rmsprop_a**0.5 + 1e-8)) * gradient_a\n",
    "        b -= (lr / (rmsprop_b**0.5 + 1e-8)) * gradient_b\n",
    "    return a, b, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamOptimizer\n",
    "\n",
    "- Momentum + RMS_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_gradient_descent(a, b, x, y, lr=1e-5, b1=0.9, b2=0.999, epsilon=1e-4):\n",
    "    prev_error = 0\n",
    "    m_a = v_a = m_b = v_b = 0\n",
    "    moment_m_a = moment_v_a = moment_m_b = moment_v_b = 0\n",
    "    t = 0\n",
    "    error = np.array([])\n",
    "    while True:\n",
    "        gradient_a, gradient_b = gradient(a, b, x, y)\n",
    "#         print(abs(mse(a, b, x, y) - prev_error))\n",
    "        if abs(MeanSquareError(a, b, x, y) - prev_error) < epsilon:\n",
    "            break\n",
    "        t += 1\n",
    "        prev_error = MeanSquareError(a, b, x, y)\n",
    "        error = np.insert(error, len(error), prev_error)\n",
    "\n",
    "        m_a = b1 * m_a + (1-b1)*gradient_a\n",
    "        v_a = b2 * v_a + (1-b2)*gradient_a**2\n",
    "        m_b = b1 * m_b + (1-b1)*gradient_b\n",
    "        v_b = b2 * v_b + (1-b2)*gradient_b**2\n",
    "        moment_m_a = m_a / (1-b1**t)\n",
    "        moment_v_a = v_a / (1-b2**t)\n",
    "        moment_m_b = m_b / (1-b1**t)\n",
    "        moment_v_b = v_b / (1-b2**t)\n",
    "        a -= (lr*moment_m_a) / (moment_v_a**0.5 + 1e-8)\n",
    "        b -= (lr*moment_m_b) / (moment_v_b**0.5 + 1e-8)\n",
    "    return a, b, error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
